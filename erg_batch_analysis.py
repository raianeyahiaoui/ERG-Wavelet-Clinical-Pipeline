# -*- coding: utf-8 -*-
"""oculusgraphy_OP_analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O_nzSyee8NzkzypdCVhZud-zrDHKGGWz
"""

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

# Step 0: Fresh environment (Colab)
!pip install --upgrade pip
!pip install PyWavelets seaborn

import os
import glob
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
from scipy.ndimage import gaussian_filter1d
from scipy.signal import find_peaks
import pywt  # Import name remains 'pywt' even though pip package is 'PyWavelets'

# Preferred modern styling
sns.set_theme(style='whitegrid')  # Use seaborn API for styling
# Alternatively: plt.style.use('seaborn-v0_8-whitegrid')  # Matplotlib compatibility style

# Step 1: Upload the Excel(s)
import os
import pandas as pd
FILE_1 = '01 Appendix 1.xlsx'
FILE_2 = '02 Appendix 2.xlsx'  # optional if present
OUTPUT_DIR = '/content/erg_outputs'
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Step 2: Inspect sheets and auto-select OP sheet
xls1 = pd.ExcelFile(FILE_1)
print("Sheets in Appendix 1:", xls1.sheet_names)

OP_SHEET_1 = None
for s in xls1.sheet_names:
    if 'Oscillatory' in s or 'OP' in s:
        OP_SHEET_1 = s
        break
if OP_SHEET_1 is None:
    OP_SHEET_1 = xls1.sheet_names[0]
print("Selected OP sheet in Appendix 1:", OP_SHEET_1)

if os.path.exists(FILE_2):
    xls2 = pd.ExcelFile(FILE_2)
    print("Sheets in Appendix 2:", xls2.sheet_names)
    OP_SHEET_2 = None
    for s in xls2.sheet_names:
        if 'Oscillatory' in s or 'OP' in s:
            OP_SHEET_2 = s
            break
    if OP_SHEET_2 is None:
        OP_SHEET_2 = xls2.sheet_names[0]
    print("Selected OP sheet in Appendix 2:", OP_SHEET_2)

# Read Appendix 1, Oscillatory Potentials
df1 = pd.read_excel(FILE_1, sheet_name=OP_SHEET_1, header=None)

# If using Appendix 2
if 'OP_SHEET_2' in globals() and OP_SHEET_2 is not None:
    df2 = pd.read_excel(FILE_2, sheet_name=OP_SHEET_2, header=None)

def detect_data_start(df, max_search_rows=30):
    for r in range(min(max_search_rows, len(df))):
        row = pd.to_numeric(df.iloc[r, :], errors='coerce')
        if np.isfinite(row).sum() >= 2:
            return r
    return 6  # fallback

def extract_time_signal_block(df, time_col_index=0, signal_col_index=2, data_guess_start=None):
    if data_guess_start is None:
        data_guess_start = detect_data_start(df)
    block = df.iloc[data_guess_start:, [time_col_index, signal_col_index]].copy()
    block = block.apply(pd.to_numeric, errors='coerce').dropna()
    time = block.iloc[:, 0].values.astype(float)
    signal = block.iloc[:, 1].values.astype(float)
    if not np.all(np.diff(time) >= 0):
        idx = np.argsort(time)
        time = time[idx]; signal = signal[idx]
    return time, signal

# Try column 2 first
time, sig = extract_time_signal_block(df1, time_col_index=0, signal_col_index=2, data_guess_start=None)

def analyze_op_signal(time, erg_signal, sigma=2.0, peak_height=0.5, peak_distance=5):
    denoised = gaussian_filter1d(erg_signal, sigma)
    noise = erg_signal - denoised
    noise_std = np.std(noise)
    snr = 20 * np.log10(np.mean(np.abs(erg_signal)) / noise_std) if noise_std > 0 else np.inf

    peaks, _ = find_peaks(denoised, height=peak_height, distance=peak_distance)
    valleys, _ = find_peaks(-denoised, height=peak_height, distance=peak_distance)

    avg_amp = 0.0; avg_latency = 0.0
    if len(peaks) and len(valleys):
        n = min(len(peaks), len(valleys))
        avg_amp = float(np.mean(denoised[peaks[:n]] - denoised[valleys[:n]]))
        avg_latency = float(np.mean(time[peaks[:n]]))

    widths = np.arange(1, 31)
    cwtmatr, freqs = pywt.cwt(denoised, widths, 'mexh')
    energy = np.sum(np.abs(cwtmatr)**2, axis=0)
    threshold = np.percentile(energy, 95)
    pattern_indices = np.where(energy > threshold)[0]

    return {
        'snr_db': snr,
        'avg_amp_uV': avg_amp,
        'avg_latency_ms': avg_latency,
        'peaks_idx': peaks,
        'valleys_idx': valleys,
        'denoised': denoised,
        'cwt': cwtmatr,
        'freqs': freqs,
        'energy': energy,
        'pattern_indices': pattern_indices
    }

res = analyze_op_signal(time, sig, sigma=2.0, peak_height=0.5, peak_distance=5)
print(f"SNR={res['snr_db']:.2f} dB | Amp={res['avg_amp_uV']:.2f} µV | Lat={res['avg_latency_ms']:.2f} ms")

def plot_per_signal_report(time, raw, res, title, out_prefix):
    fig, axs = plt.subplots(4, 1, figsize=(12, 16))

    axs[0].plot(time, raw, label='Original', alpha=0.85)
    axs[0].plot(time, res['denoised'], label='Denoised', linewidth=1.5)
    axs[0].set_title(f'{title} - ERG OP signal'); axs[0].set_xlabel('Time (ms)'); axs[0].set_ylabel('Amplitude (µV)')
    axs[0].legend()

    axs[1].plot(time, res['denoised'], color='tab:blue')
    if len(res['peaks_idx']): axs[1].plot(time[res['peaks_idx']], res['denoised'][res['peaks_idx']], 'x', color='red', label='Peaks')
    if len(res['valleys_idx']): axs[1].plot(time[res['valleys_idx']], res['denoised'][res['valleys_idx']], 'o', color='green', label='Valleys')
    axs[1].set_title('Peak/Valley detection'); axs[1].legend()

    axs[2].plot(time, res['energy'], color='tab:purple')
    if len(res['pattern_indices']): axs[2].scatter(time[res['pattern_indices']], res['energy'][res['pattern_indices']], color='red', s=18, label='Hotspots')
    axs[2].set_title('CWT energy across time'); axs[2].set_xlabel('Time (ms)'); axs[2].set_ylabel('Energy'); axs[2].legend()

    axs[3].imshow(np.abs(res['cwt']), extent=[time.min(), time.max(), res['freqs'].min(), res['freqs'].max()],
                  cmap='PRGn', aspect='auto', origin='lower')
    axs[3].set_title('Scalogram (CWT - Mexican hat)'); axs[3].set_ylabel('Scale (higher = lower freq)'); axs[3].set_xlabel('Time (ms)')

    plt.tight_layout()
    fig.savefig(os.path.join(OUTPUT_DIR, f'{out_prefix}_report.pdf'), dpi=300)
    plt.close(fig)

plot_per_signal_report(time, sig, res, title=f'{OP_SHEET_1}, Col 2', out_prefix='appendix1_col2')

summary_rows = []

def batch_analyze(df, file_label='Appendix1', time_col_index=0, signal_cols=range(2, 12), data_guess_start=None):
    for c in signal_cols:
        try:
            time, sig = extract_time_signal_block(df, time_col_index=time_col_index,
                                                  signal_col_index=c, data_guess_start=data_guess_start)
            res = analyze_op_signal(time, sig, sigma=2.0, peak_height=0.5, peak_distance=5)
            summary_rows.append({
                'file': file_label,
                'signal_col': c,
                'snr_db': res['snr_db'],
                'avg_amp_uV': res['avg_amp_uV'],
                'avg_latency_ms': res['avg_latency_ms'],
                'n_peaks': len(res['peaks_idx']),
                'n_hotspots': len(res['pattern_indices'])
            })
            plot_per_signal_report(time, sig, res, title=f'{file_label}, Col {c}', out_prefix=f'{file_label}_col{c}')
            print(f"[OK] {file_label} col {c}: SNR={res['snr_db']:.2f} dB, Amp={res['avg_amp_uV']:.2f} µV, Lat={res['avg_latency_ms']:.2f} ms")
        except Exception as e:
            print(f"[Skip] {file_label} col {c}: {e}")

# Run batch on Appendix 1
batch_analyze(df1, file_label='Appendix1', time_col_index=0, signal_cols=range(2, 12), data_guess_start=None)

# Optional: Appendix 2
if 'df2' in globals():
    batch_analyze(df2, file_label='Appendix2', time_col_index=0, signal_cols=range(2, 12), data_guess_start=None)

summary_df = pd.DataFrame(summary_rows)
summary_df.to_csv(os.path.join(OUTPUT_DIR, 'erg_op_summary.csv'), index=False)
summary_df.head(10)

# Histograms
fig, axs = plt.subplots(1, 2, figsize=(12, 4))
axs[0].hist(summary_df['avg_amp_uV'].dropna(), bins=12, color='tab:blue', alpha=0.85)
axs[0].set_title('OP average amplitudes'); axs[0].set_xlabel('Amplitude (µV)'); axs[0].set_ylabel('Count')
axs[1].hist(summary_df['avg_latency_ms'].dropna(), bins=12, color='tab:orange', alpha=0.85)
axs[1].set_title('OP average latencies'); axs[1].set_xlabel('Latency (ms)'); axs[1].set_ylabel('Count')
plt.tight_layout(); plt.savefig(os.path.join(OUTPUT_DIR, 'histograms_amp_latency.png'), dpi=200); plt.show()

# Correlation heatmap
import seaborn as sns
metrics = summary_df[['snr_db', 'avg_amp_uV', 'avg_latency_ms', 'n_peaks', 'n_hotspots']].copy()
corr = metrics.corr(numeric_only=True)
plt.figure(figsize=(6, 5))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', square=True)
plt.title('Correlation heatmap of ERG OP metrics')
plt.tight_layout(); plt.savefig(os.path.join(OUTPUT_DIR, 'metrics_correlation_heatmap.png'), dpi=200); plt.show()

# Best-SNR scalogram snapshot
if len(summary_df) > 0:
    best_idx = summary_df['snr_db'].idxmax()
    best_row = summary_df.iloc[best_idx]
    file_label = best_row['file']; col_idx = int(best_row['signal_col'])
    df_sel = df1 if file_label == 'Appendix1' else df2
    time_b, sig_b = extract_time_signal_block(df_sel, time_col_index=0, signal_col_index=col_idx, data_guess_start=None)
    res_b = analyze_op_signal(time_b, sig_b, sigma=2.0, peak_height=0.5, peak_distance=5)

    plt.figure(figsize=(10, 6))
    plt.imshow(np.abs(res_b['cwt']), extent=[time_b.min(), time_b.max(), res_b['freqs'].min(), res_b['freqs'].max()],
               cmap='PRGn', aspect='auto', origin='lower')
    plt.title(f'Scalogram (best SNR) - {file_label} col {col_idx}')
    plt.ylabel('Scale (higher = lower freq)'); plt.xlabel('Time (ms)')
    plt.colorbar(label='|CWT|')
    plt.tight_layout(); plt.savefig(os.path.join(OUTPUT_DIR, f'{file_label}_col{col_idx}_scalogram.png'), dpi=200); plt.show()

mean_amp = float(summary_df['avg_amp_uV'].mean())
mean_latency = float(summary_df['avg_latency_ms'].mean())
mean_snr = float(summary_df['snr_db'].mean())
print(f"Cohort mean amplitude: {mean_amp:.2f} µV")
print(f"Cohort mean latency: {mean_latency:.2f} ms")
print(f"Cohort mean SNR: {mean_snr:.2f} dB")

